import { EventRelay } from "lifecycle-utils";
import { Token } from "../types.js";
import { ModelTypeDescription } from "../utils/getBin.js";
export type LlamaModelOptions = {
    /** path to the model on the filesystem */
    modelPath: string;
    /** number of layers to store in VRAM */
    gpuLayers?: number;
    /** only load the vocabulary, no weights */
    vocabOnly?: boolean;
    /** use mmap if possible */
    useMmap?: boolean;
    /** force system to keep model in RAM */
    useMlock?: boolean;
};
export declare class LlamaModel {
    readonly onDispose: EventRelay<void>;
    /**
     * > options source:
     * > [github:ggerganov/llama.cpp/llama.h](
     * > https://github.com/ggerganov/llama.cpp/blob/05816027d649f977468fc804cdb54e99eac246d1/llama.h#L161) (`struct llama_model_params`)
     * @param options
     * @param options.modelPath - path to the model on the filesystem
     * @param [options.gpuLayers] - number of layers to store in VRAM
     * @param [options.vocabOnly] - only load the vocabulary, no weights
     * @param [options.useMmap] - use mmap if possible
     * @param [options.useMlock] - force system to keep model in RAM
     */
    constructor({ modelPath, gpuLayers, vocabOnly, useMmap, useMlock }: LlamaModelOptions);
    dispose(): void;
    /** @hidden */
    [Symbol.dispose](): void;
    get disposed(): boolean;
    get tokens(): LlamaModelTokens;
    get filename(): string | undefined;
    /** Transform text into tokens that can be fed to the model */
    tokenize(text: string): Token[];
    /** Transform tokens into text */
    detokenize(tokens: readonly Token[]): string;
    /** @hidden `ModelTypeDescription` type alias is too long in the documentation */
    get typeDescription(): ModelTypeDescription;
    /** The context size the model was trained on */
    get trainContextSize(): number;
    static get systemInfo(): string;
}
export declare class LlamaModelTokens {
    private constructor();
    /**
     * @returns infill tokens
     */
    get infill(): LlamaModelInfillTokens;
    /**
     * @returns The BOS (Beginning Of Sequence) token.
     */
    get bos(): Token | null;
    /**
     * @returns The EOS (End Of Sequence) token.
     */
    get eos(): Token | null;
    /**
     * @returns The NL (New Line) token.
     */
    get nl(): Token | null;
    /**
     * @returns The BOS (Beginning Of Sequence) token as a string.
     */
    get bosString(): string | null;
    /**
     * @returns The EOS (End Of Sequence) token as a string.
     */
    get eosString(): string | null;
    /**
     * @returns The NL (New Line) token as a string.
     */
    get nlString(): string | null;
}
export declare class LlamaModelInfillTokens {
    private constructor();
    /**
     * @returns The beginning of infill prefix token.
     */
    get prefix(): Token | null;
    /**
     * @returns The beginning of infill middle token.
     */
    get middle(): Token | null;
    /**
     * @returns The beginning of infill suffix token.
     */
    get suffix(): Token | null;
    /**
     * @returns End of infill middle token (End Of Text).
     */
    get eot(): Token | null;
    /**
     * @returns The beginning of infill prefix token as a string.
     */
    get prefixString(): string | null;
    /**
     * @returns The beginning of infill middle token as a string.
     */
    get middleString(): string | null;
    /**
     * @returns The beginning of infill suffix token as a string.
     */
    get suffixString(): string | null;
    /**
     * @returns End of infill middle token (End Of Text) as a string.
     */
    get eotString(): string | null;
}
