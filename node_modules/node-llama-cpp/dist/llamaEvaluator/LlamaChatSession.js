import { DisposeAggregator, DisposedError, EventRelay, withLock } from "lifecycle-utils";
import { defaultChatSystemPrompt } from "../config.js";
import { AbortError } from "../AbortError.js";
import { GeneralChatPromptWrapper } from "../chatWrappers/GeneralChatPromptWrapper.js";
import { resolveChatWrapperBasedOnModel } from "../chatWrappers/resolveChatWrapperBasedOnModel.js";
import { generateContextTextFromConversationHistory } from "../chatWrappers/generateContextTextFromConversationHistory.js";
import { removeNullFields } from "../utils/removeNullFields.js";
import { LlamaModel } from "./LlamaModel.js";
import { LlamaGrammarEvaluationState } from "./LlamaGrammarEvaluationState.js";
const UNKNOWN_UNICODE_CHAR = "\ufffd";
export class LlamaChatSession {
    /** @internal */ _systemPrompt;
    /** @internal */ _printLLamaSystemInfo;
    /** @internal */ _promptWrapper;
    /** @internal */ _disposeAggregator = new DisposeAggregator();
    /** @internal */ _autoDisposeSequence;
    /** @internal */ _promptIndex = 0;
    /** @internal */ _initialized = false;
    /** @internal */ _lastStopString = null;
    /** @internal */ _lastStopStringSuffix = null;
    /** @internal */ _conversationHistoryToLoad = null;
    /** @internal */ _sequence;
    onDispose = new EventRelay();
    /**
     * @param options
     */
    constructor({ contextSequence, printLLamaSystemInfo = false, promptWrapper = "auto", systemPrompt = defaultChatSystemPrompt, conversationHistory, autoDisposeSequence = true }) {
        if (contextSequence.disposed)
            throw new DisposedError();
        this._sequence = contextSequence;
        this._printLLamaSystemInfo = printLLamaSystemInfo;
        this._systemPrompt = systemPrompt;
        this._conversationHistoryToLoad = (conversationHistory != null && conversationHistory.length > 0)
            ? conversationHistory
            : null;
        this._autoDisposeSequence = autoDisposeSequence;
        this._disposeAggregator.add(this._sequence.onDispose.createListener(() => {
            this.dispose();
        }));
        this._disposeAggregator.add(this.onDispose.dispatchEvent);
        if (promptWrapper === "auto") {
            const chatWrapper = resolveChatWrapperBasedOnModel({
                bosString: contextSequence.model.tokens.bosString,
                filename: contextSequence.model.filename,
                typeDescription: contextSequence.model.typeDescription
            });
            if (chatWrapper != null)
                this._promptWrapper = new chatWrapper();
            else
                this._promptWrapper = new GeneralChatPromptWrapper();
        }
        else
            this._promptWrapper = promptWrapper;
    }
    dispose({ disposedSequence = this._autoDisposeSequence } = {}) {
        if (this._sequence == null)
            return;
        if (disposedSequence)
            this._sequence.dispose();
        this._sequence = null;
        this._disposeAggregator.dispose();
    }
    /** @hidden */
    [Symbol.dispose]() {
        return this.dispose();
    }
    get disposed() {
        return this._sequence == null;
    }
    get initialized() {
        return this._initialized;
    }
    get sequence() {
        if (this._sequence == null)
            throw new DisposedError();
        return this._sequence;
    }
    get context() {
        return this.sequence.context;
    }
    get model() {
        return this.sequence.model;
    }
    async init() {
        await withLock(this, "init", async () => {
            this._ensureNotDisposed();
            if (this._initialized)
                return;
            if (this._printLLamaSystemInfo)
                console.log("Llama system info", LlamaModel.systemInfo);
            this._initialized = true;
        });
    }
    /**
     * @param prompt
     * @param [options]
     */
    async prompt(prompt, { onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix = false, repeatPenalty } = {}) {
        const { text } = await this.promptWithMeta(prompt, {
            onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix, repeatPenalty
        });
        return text;
    }
    /**
     * @param prompt
     * @param [options]
     */
    async promptWithMeta(prompt, { onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix = false, repeatPenalty } = {}) {
        this._ensureNotDisposed();
        if (!this.initialized)
            await this.init();
        return await withLock(this, "prompt", async () => {
            this._ensureNotDisposed();
            if (this._sequence == null)
                throw new DisposedError();
            let promptText = "";
            if (this._promptIndex == 0 && this._conversationHistoryToLoad != null) {
                const { text, stopString, stopStringSuffix } = generateContextTextFromConversationHistory(this._promptWrapper, this._conversationHistoryToLoad, {
                    systemPrompt: this._systemPrompt,
                    currentPromptIndex: this._promptIndex,
                    lastStopString: this._lastStopString,
                    lastStopStringSuffix: this._promptIndex == 0
                        ? (this._sequence.prependBos
                            ? this._sequence.context.model.tokens.bosString
                            : null)
                        : this._lastStopStringSuffix
                });
                promptText += text;
                this._lastStopString = stopString;
                this._lastStopStringSuffix = stopStringSuffix;
                this._promptIndex += this._conversationHistoryToLoad.length;
                this._conversationHistoryToLoad = null;
            }
            promptText += this._promptWrapper.wrapPrompt(prompt, {
                systemPrompt: this._systemPrompt,
                promptIndex: this._promptIndex,
                lastStopString: this._lastStopString,
                lastStopStringSuffix: this._promptIndex == 0
                    ? (this._sequence.prependBos
                        ? this._sequence.context.model.tokens.bosString
                        : null)
                    : this._lastStopStringSuffix
            });
            this._promptIndex++;
            this._lastStopString = null;
            this._lastStopStringSuffix = null;
            const { text, stopReason, stopString, stopStringSuffix } = await this._evalTokens(this._sequence.context.model.tokenize(promptText), {
                onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix,
                repeatPenalty: repeatPenalty == false ? { lastTokens: 0 } : repeatPenalty
            });
            this._lastStopString = stopString;
            this._lastStopStringSuffix = stopStringSuffix;
            return {
                text,
                stopReason,
                stopString,
                stopStringSuffix
            };
        });
    }
    /** @internal */
    async _evalTokens(tokens, { onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix = false, repeatPenalty: { lastTokens: repeatPenaltyLastTokens = 64, punishTokensFilter, penalizeNewLine, penalty, frequencyPenalty, presencePenalty } = {} } = {}) {
        if (this._sequence == null)
            throw new DisposedError();
        let stopStrings = this._promptWrapper.getStopStrings();
        if (grammar != null)
            stopStrings = stopStrings.concat(grammar.stopStrings);
        const stopStringIndexes = Array(stopStrings.length).fill(0);
        const skippedChunksQueue = [];
        const res = [];
        const grammarEvaluationState = grammar != null
            ? new LlamaGrammarEvaluationState({ grammar })
            : undefined;
        const repeatPenaltyEnabled = repeatPenaltyLastTokens > 0;
        let stopReason = "eosToken";
        const getPenaltyTokens = () => {
            let punishTokens = res.slice(-repeatPenaltyLastTokens);
            if (punishTokensFilter != null)
                punishTokens = punishTokensFilter(punishTokens);
            if (!penalizeNewLine) {
                const nlToken = this.sequence.context.model.tokens.nl;
                if (nlToken != null)
                    punishTokens = punishTokens.filter(token => token !== nlToken);
            }
            return Uint32Array.from(punishTokens);
        };
        const evaluationIterator = this._sequence.evaluate(tokens, removeNullFields({
            temperature, topK, topP, grammarEvaluationState,
            repeatPenalty: !repeatPenaltyEnabled ? undefined : {
                punishTokens: getPenaltyTokens,
                penalty,
                frequencyPenalty,
                presencePenalty
            }
        }));
        for await (const chunk of evaluationIterator) {
            if (signal?.aborted)
                throw new AbortError();
            if (this._sequence == null)
                throw new DisposedError();
            const tokenStr = this._sequence.context.model.detokenize([chunk]);
            const { shouldReturn, skipTokenEvent, stopString, stopStringSuffix } = this._checkStopString(tokenStr, stopStrings, stopStringIndexes);
            if (shouldReturn) {
                skippedChunksQueue.push(chunk);
                const skippedChunksText = skippedChunksQueue.length > 0
                    ? this._sequence.context.model.detokenize(skippedChunksQueue)
                    : "";
                let [queuedTextBeforeStopString] = skippedChunksText.split(stopString);
                if (grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix)
                    queuedTextBeforeStopString = queuedTextBeforeStopString.trimEnd();
                if (queuedTextBeforeStopString.length > 0) {
                    const beforeStopStringTokens = Array.from(this._sequence.context.model.tokenize(queuedTextBeforeStopString));
                    res.push(...beforeStopStringTokens);
                    onToken?.(beforeStopStringTokens);
                    skippedChunksQueue.length = 0;
                }
                stopReason = "stopString";
                return {
                    text: this._sequence.context.model.detokenize(res),
                    stopReason,
                    stopString,
                    stopStringSuffix
                };
            }
            // if the token is unknown, it means it's not complete character
            if (tokenStr === UNKNOWN_UNICODE_CHAR || skipTokenEvent || ((grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix) && tokenStr.trim() === "")) {
                skippedChunksQueue.push(chunk);
                continue;
            }
            if (skippedChunksQueue.length > 0) {
                res.push(...skippedChunksQueue);
                onToken?.(skippedChunksQueue);
                skippedChunksQueue.length = 0;
            }
            res.push(chunk);
            onToken?.([chunk]);
            if (maxTokens != null && maxTokens > 0 && res.length >= maxTokens) {
                stopReason = "maxTokens";
                break;
            }
        }
        if (this._sequence == null)
            throw new DisposedError();
        let resText = this._sequence.context.model.detokenize(res);
        if (grammar?.trimWhitespaceSuffix || trimWhitespaceSuffix)
            resText = resText.trimEnd();
        return {
            text: resText,
            stopReason,
            stopString: null,
            stopStringSuffix: null
        };
    }
    /** @internal */
    _checkStopString(tokenStr, stopStrings, stopStringIndexes) {
        let skipTokenEvent = false;
        for (let stopStringIndex = 0; stopStringIndex < stopStrings.length; stopStringIndex++) {
            const stopString = stopStrings[stopStringIndex];
            let localShouldSkipTokenEvent = false;
            let i = 0;
            for (; i < tokenStr.length && stopStringIndexes[stopStringIndex] !== stopString.length; i++) {
                if (tokenStr[i] === stopString[stopStringIndexes[stopStringIndex]]) {
                    stopStringIndexes[stopStringIndex]++;
                    localShouldSkipTokenEvent = true;
                }
                else {
                    stopStringIndexes[stopStringIndex] = 0;
                    localShouldSkipTokenEvent = false;
                }
            }
            if (stopStringIndexes[stopStringIndex] === stopString.length) {
                return {
                    shouldReturn: true,
                    stopString,
                    stopStringSuffix: tokenStr.length === i
                        ? null
                        : tokenStr.slice(i)
                };
            }
            skipTokenEvent ||= localShouldSkipTokenEvent;
        }
        return { skipTokenEvent };
    }
    /** @internal */
    _ensureNotDisposed() {
        if (this._sequence == null)
            throw new DisposedError();
    }
}
//# sourceMappingURL=LlamaChatSession.js.map