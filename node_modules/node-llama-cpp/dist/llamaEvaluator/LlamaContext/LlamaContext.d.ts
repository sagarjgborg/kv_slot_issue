import { EventRelay } from "lifecycle-utils";
import { Token } from "../../types.js";
import { LlamaModel } from "../LlamaModel.js";
import { LlamaGrammarEvaluationState } from "../LlamaGrammarEvaluationState.js";
import { ContextShiftOptions, ContextTokensDeleteRange, EvaluationPriority, LlamaContextOptions, LlamaContextSequenceRepeatPenalty, TokenPriority } from "./types.js";
export declare class LlamaContext {
    readonly onDispose: EventRelay<void>;
    /**
     * @param options
     */
    constructor({ model, sequences, seed, contextSize, batchSize, f16Kv, logitsAll, embedding, threads, batching: { dispatchSchedule: batchingDispatchSchedule, itemsPrioritizingStrategy: batchingItemsPrioritizingStrategy } }: LlamaContextOptions);
    dispose(): void;
    /** @hidden */
    [Symbol.dispose](): void;
    get disposed(): boolean;
    get model(): LlamaModel;
    get contextSize(): number;
    get batchSize(): number;
    getAllocatedContextSize(): number;
    get totalSequences(): number;
    get sequencesLeft(): number;
    /**
     * Before calling this method, make sure to call `sequencesLeft` to check if there are any sequences left.
     * When there are no sequences left, this method will throw an error.
     * @param [options]
     */
    getSequence({ prependBos, contextShift: { size: contextShiftSize, strategy: contextShiftStrategy } }?: {
        prependBos?: boolean;
        contextShift?: ContextShiftOptions;
    }): LlamaContextSequence;
    dispatchPendingBatch(): void;
}
export declare class LlamaContextSequence {
    readonly onDispose: EventRelay<void>;
    private constructor();
    dispose(): void;
    /** @hidden */
    [Symbol.dispose](): void;
    get disposed(): boolean;
    get context(): LlamaContext;
    get model(): LlamaModel;
    get prependBos(): boolean;
    get nextTokenIndex(): number;
    get contextTokens(): Token[];
    get contextTokenPriorities(): number[];
    /**
     * Clear the history of the sequence.
     * If `prependBos` was enabled, the BOS token will be prepended to the sequence again.
     */
    clearHistory(): Promise<void>;
    /**
     * Erase context tokens in the provided ranges to free up space for new tokens to be generated.
     * the start and end of each range are exclusive.
     * For example, the range `{start: 0, end: 1}` will remove the token at the `0` index only.
     */
    eraseContextTokenRanges(ranges: ContextTokensDeleteRange[]): Promise<void>;
    /**
     * @param tokens
     * @param [options]
     */
    evaluate(tokens: Token[], { temperature, topK, topP, grammarEvaluationState, repeatPenalty, evaluationPriority, tokenPriority }?: {
        temperature?: number;
        topK?: number;
        topP?: number;
        grammarEvaluationState?: LlamaGrammarEvaluationState;
        repeatPenalty?: LlamaContextSequenceRepeatPenalty;
        /**
         * When a lot of tokens are queued for the next batch, more than the configured `batchSize`, the tokens for each sequence will be
         * evaluated based on the strategy chosen for the context.
         * By default, the `"maximumParallelism"` strategy is used, which will try to evaluate as many sequences in parallel as possible,
         * but at some point, it'll have to choose which sequences to evaluate more tokens of, so it'll prioritize the sequences with the
         * highest evaluation priority.
         * Also, a custom strategy can be used to prioritize the sequences differently, but generally, the higher the evaluation priority
         * is, the more likely and more tokens will be evaluated for that sequence in the next queued batch.
         */
        evaluationPriority?: EvaluationPriority;
        /**
         * When the context is full, tokens will be erased based on the context shift strategy chosen.
         * By default, the lowest priority tokens at the beginning of the context will be erased.
         * To mark the priority of the evaluated tokens, use this option.
         * The higher the priority is, the less likely it will be erased.
         * The default priority is `1`.
         */
        tokenPriority?: TokenPriority | TokenPriority[];
    }): AsyncGenerator<Token, void>;
    /**
     * Evaluate the provided tokens into the context sequence without generating new tokens.
     * @param tokens
     * @param [options]
     */
    evaluateWithoutGeneratingNewTokens(tokens: Token[], { evaluationPriority, tokenPriority }?: {
        grammarEvaluationState?: LlamaGrammarEvaluationState;
        /**
         * When a lot of tokens are queued for the next batch, more than the configured `batchSize`, the tokens for each sequence will be
         * evaluated based on the strategy chosen for the context.
         * By default, the `"maximumParallelism"` strategy is used, which will try to evaluate as many sequences in parallel as possible,
         * but at some point, it'll have to choose which sequences to evaluate more tokens of, so it'll prioritize the sequences with the
         * highest evaluation priority.
         * Also, a custom strategy can be used to prioritize the sequences differently, but generally, the higher the evaluation priority
         * is, the more likely and more tokens will be evaluated for that sequence in the next queued batch.
         */
        evaluationPriority?: EvaluationPriority;
        /**
         * When the context is full, tokens will be erased based on the context shift strategy chosen.
         * By default, the lowest priority tokens at the beginning of the context will be erased.
         * To mark the priority of the evaluated tokens, use this option.
         * The higher the priority is, the less likely it will be erased.
         * The default priority is `1`.
         */
        tokenPriority?: TokenPriority | TokenPriority[];
    }): Promise<void>;
}
